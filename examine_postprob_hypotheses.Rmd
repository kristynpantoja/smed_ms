---
title: "Examining Posterior Probability of Hypotheses Calculation"
author: "Kristyn Pantoja"
date: "9/9/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r sources_data_loading, include=FALSE}
library(expm)
library(matrixStats)
library(scatterplot3d)
library(knitr)
library(mvtnorm)

home = "/Users/kristyn/Documents/research/smed_ms"
functions_home = paste(home, "/functions", sep="")

source(paste(functions_home, "/construct_design_matrix.R", sep = ""))
source(paste(functions_home, "/posterior_variance.R", sep = ""))
source(paste(functions_home, "/variance_marginal_y.R", sep = ""))
source(paste(functions_home, "/wasserstein_distance.R", sep = ""))
source(paste(functions_home, "/charge_function_q.R", sep = ""))
source(paste(functions_home, "/total_potential_energy_criteria.R", sep = ""))
source(paste(functions_home, "/fast_criteria.R", sep = ""))
source(paste(functions_home, "/oneatatime_criteria.R", sep = ""))
source(paste(functions_home, "/simulate_y.R", sep = ""))
source(paste(functions_home, "/postprob_hypotheses.R", sep = ""))
source(paste(functions_home, "/postmean_mse_closedform.R", sep = ""))
source(paste(functions_home, "/postmean_mse_mc.R", sep = ""))

load(paste(home, "/designs_slides.RData", sep = ""))

# parameters/settings
mean_beta0 = c(0, 0, 0) # null model prior mean on beta
mean_beta1 = c(0, 0.2, -0.2, 0.2, -0.2) # alternative model prior mean on beta
var_mean0 = diag(c(0.005, 0.005, 0.005)) # null model prior variance on beta
var_mean1 = diag(c(0.005, 0.005, 0.005, 0.005, 0.005)) # alternative model prior variance on beta
var_e = 0.005 # variance on error
xmin = 0
xmax = 1
f0 = function(x) mean_beta0[1] + mean_beta0[2] * x[1] + mean_beta0[3] * x[2] # null
f1 = function(x) mean_beta1[1] + mean_beta1[2] * x[1] + mean_beta1[3] * x[1]^2 + mean_beta1[4] * x[2] + mean_beta1[3] * x[2]^2
N = 100
type = c(4, 5) # type (4,5) model is my indicator to the function of what kind of models are being compared so that marginal variance is correctly computed
p = 3


# for one-at-a-time algorithm, k = 1
numCandidates = 10e3
k = c(1, 4, 50)
one_at_a_time_k1 = all_designs$one_at_a_time_k1

# for one-at-a-time algorithm, k = 1
one_at_a_time_k4 = all_designs$one_at_a_time_k4

# for one-at-a-time algorithm, k = 50
one_at_a_time_k50 = all_designs$one_at_a_time_k50

# for fast algorithm, S = 5
S = 5
fast_S5 = all_designs$fast_S5

# D-optimal
#rep1 = rep(c(0, 1), each = 25); rep2 = rep(0, each = 50); rep3 = rep(1, each = 50)
#doptimal = rbind(cbind(rep1, rep2), cbind(rep1, rep3))
doptimal = all_designs$doptimal

# Space-filling (grid)
#axis1 = seq(from = xmin, to = xmax, length.out = 10); axis2 = seq(from = xmin, to = xmax, length.out = 10)
#space_filing = cbind(rep(axis1, each = 10), rep(axis2, times = 10))
space_filling = all_designs$space_filling

# Random
#set.seed(1)
#axis1 = runif(100, xmin, xmax); axis2 = runif(100, xmin, xmax)
#random_design = cbind(axis1, axis2)
random_design = all_designs$random_design

numSims = 100
```

# First, Review Posterior Probabilities of Hypotheses

## Posterior Probabilities of Hypotheses

- Posterior Probability of model $H_\ell, \ell \in 1, ..., M$:
$$
P(H_\ell | y, X) = \frac{\pi_\ell f(y | H_\ell, X)}{\sum_{m = 1}^M \pi_m f(y | H_m, X)}
$$
where $\pi_m$ is the prior on  $H_m$ (typically $\pi_m = \frac{1}{M}$), and $f(y | H_m, X)$ is the model evidence.
- $P(H_\ell | y, X)$ tells which hypothesis is more likely to give the correct model.
- $E[P(H_\ell | y, X) | H_r, X]$ may be estimated using MC approximation from simulated responses $y$ under a chosen hypothesis $H_r$.
- $E[P(H_\ell | y, \mathbf{D}) | H_r, \mathbf{D}]$ can be used to evaluate a design $\mathbf{D}$'s ability to distinguish hypotheses


## Estimate Expected Posterior Probability of a Hypothesis

Estimate the expected posterior probability of hypothesis $H_\ell$ for $J$ simulations of $Y$ under $H_r$, given design $\mathbf{D} = \{ x_1, ..., x_N \}$:

1. For $j = 1, \dots, J$:
    1. Draw $\beta \sim N(\mu_r, \nu_r^2)$
    2. Draw $y_i^{(j)} | \mathbf{D} \sim N(\mathbf{x}_i \beta, \sigma_r^2)$, $\forall \mathbf{x}_i \in \mathbf{D}$
    3. $\forall m \in \{ 1, ..., M \}$, calculate model evidences $f(y | H_m, \mathbf{D})$
        - $f(y | H_m, \mathbf{D})$ is the marginal likelihood $N(\mathbf{D} \mu_m, \sigma^2 I + \mathbf{D}V \mathbf{D}^T)$ evaluated at $y$ and $\mathbf{D}$.
    4. Calculate the posterior probability of $H_\ell$, $P(H_\ell | y^{(j)}, \mathbf{D})$, from simulation $j$
$$
			P(H_\ell | y^{(j)}, \mathbf{D}) = \frac{\pi_\ell f(y^{(j)} | H_\ell, \mathbf{D})}{\sum_{m = 1}^M \pi_m f(y^{(j)} | H_m, \mathbf{D})}
$$
2. Average the estimated posterior probabilities of $H_\ell$ over $\forall j$ to obtain MC estimate of $E[P(H_\ell | y, \mathbf{D}) | H_r, \mathbf{D}]$

## What We Expect

- We expect that, by how MED is constructed, $E[P(H_\ell | y, \mathbf{D}_{MED}) | H_\ell, \mathbf{D}_{MED}] > E[P(H_\ell | y, \mathbf{D}_{space}) | H_\ell, \mathbf{D}_{space}]$. In other words, we want to show that the expected posterior probability of the true hypothesis (i.e. the hypothesis from which the simulated data was generated) is higher when the design is the MED design, $\mathbf{D}_{MED}$, than when the design is another design, such as the space-filling design, $\mathbf{D}_{space}$.
- We expect that this is the case because we expect that the average density $E[f(y | H_\ell, \mathbf{D}_{MED})]$ over simulated data $y | H_\ell, \mathbf{D}_{MED}$ is higher than the average density $E[f(y | H_\ell, \mathbf{D}_{space})]$ over simulated data $y | H_\ell, \mathbf{D}_{space}$.

# Now, for the problem...

## What We See

- The problem is that we don't see $E[P(H_\ell | y, \mathbf{D}_{MED}) | H_\ell, \mathbf{D}_{MED}] > E[P(H_\ell | y, \mathbf{D}_{space}) | H_\ell, \mathbf{D}_{space}]$ (in fact we see the opposite, $E[P(H_\ell | y, \mathbf{D}_{MED}) | H_\ell, \mathbf{D}_{MED}] < E[P(H_\ell | y, \mathbf{D}_{space}) | H_\ell, \mathbf{D}_{space}]$, which we don't want!).
    - Notice that the first row is $E[P(H_0 | y, \mathbf{D}) | H_0, \mathbf{D}]$ where MED does worse than space-filling
    - And the second-to-last row is $E[P(H_1 | y, \mathbf{D}) | H_1, \mathbf{D}]$ and again MED does worse than space-filling
    - This isn't at all what we expected or wanted!

```{r, echo = FALSE}
numSims = 100
fast_exppostprobs = suppressWarnings(calcExpPostProbH_2d(fast_S5, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
space_exppostprobs = suppressWarnings(calcExpPostProbH_2d(space_filling, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbsHi = data.frame(cbind(fast_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")], 
                                          space_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")]))
colnames(expectedPosteriorProbsHi) = c("Fast","Space")
rownames(expectedPosteriorProbsHi) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H1,D]")
round(expectedPosteriorProbsHi,5)
```

- (In case you're wondering,) the full table of posterior probabilities is:
```{r, echo = FALSE}
numSims = 1000
# fast_exppostprobs = suppressWarnings(calcExpPostProbH_2d(fast_S5, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
# space_exppostprobs = suppressWarnings(calcExpPostProbH_2d(space_filling, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbs = data.frame(cbind(fast_exppostprobs, space_exppostprobs))
colnames(expectedPosteriorProbs) = c("Fast","Space")
rownames(expectedPosteriorProbs) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H0,D]", "E[BF01 | H0,D]", 
                                     "E[P(H0|Y,D)|H1,D]", "E[P(H1|Y,D)|H1,D]", "E[BF01|H1,D]")
round(expectedPosteriorProbs,4)
```

- This matters, because the evidence for the other hypothesis shouldn't be higher for MED than for space-filling design. At least, that is not an intended result.


## Why This Might Be Happening

- I think this is explained by the strange occurrence that, while $E[f(y | H_\ell, \mathbf{D}_{MED})] > E[f(y | H_\ell, \mathbf{D}_{space})]$, we also see $E[f(y^{(j)} | H_{-\ell}, \mathbf{D}_{MED})] > E[f(y^{(j)} | H_{-\ell}, \mathbf{D}_{space})]$ as well! For example, let $\ell = 0$:
    1. We simulate some $Y|H_0, \mathbf{D}$ for $\mathbf{D} = \mathbf{D}_{MED}$
    2. We calculate the model evidence for each of the data, given each of the models from the hypotheses $H_0, H_1$ for this simulated data
    3. We simulate some $Y|H_0, \mathbf{D}$ for $\mathbf{D} = \mathbf{D}_{space}$
    4. We calculate the model evidence for each of the data, given each of the models from the hypotheses $H_0, H_1$ for this simulated data, as well.

```{r}
# fast MED
fast_simY_H0 = simulateY(fast_S5, N, mean_beta0, var_mean0, var_e, numSims, type = type[1])
fast_simEvidenceH0_YH0 = rep(NA, numSims) # added for bf01 calc
fast_simEvidenceH1_YH0 = rep(NA, numSims)
for(j in 1:numSims){
  Y = fast_simY_H0[, j]
  fast_simEvidenceH0_YH0[j] = model_evidence(Y, fast_S5, N, mean_beta0, 
                                             var_mean0, var_e, type = type[1])
  fast_simEvidenceH1_YH0[j] = model_evidence(Y, fast_S5, N, mean_beta1, 
                                             var_mean1, var_e, type = type[2])
}
# space-filling
space_simY_H0 = simulateY(space_filling, N, mean_beta0, var_mean0, var_e, numSims, type = type[1])
space_simEvidenceH0_YH0 = rep(NA, numSims) # added for bf01 calc
space_simEvidenceH1_YH0 = rep(NA, numSims)
for(j in 1:numSims){
  Y = space_simY_H0[, j]
  space_simEvidenceH0_YH0[j] = model_evidence(Y, space_filling, N, mean_beta0, 
                                              var_mean0, var_e, type = type[1])
  space_simEvidenceH1_YH0[j] = model_evidence(Y, space_filling, N, mean_beta1, 
                                              var_mean1, var_e, type = type[2])
}
```

- So $E[f(y | H_\ell, \mathbf{D}_{MED})] > E[f(y | H_\ell, \mathbf{D}_{space})]$:
```{r}
mean(fast_simEvidenceH0_YH0) > mean(space_simEvidenceH0_YH0)
mean(fast_simEvidenceH0_YH0) * 10e-55 # E[f(y | H0, D_MED)] for y generated from H0
mean(space_simEvidenceH0_YH0) * 10e-55 # E[f(y | H0, D_space)] for y generated from H0
```

- But also $E[f(y | H_{-\ell}, \mathbf{D}_{MED})] > E[f(y | H_{-\ell}, \mathbf{D}_{space})]$, which shouldn't be happening.
```{r}
mean(fast_simEvidenceH1_YH0) > mean(space_simEvidenceH1_YH0)
mean(fast_simEvidenceH1_YH0) * 10e-55 # E[f(y | H1, D_MED)] for y generated from H0
mean(space_simEvidenceH1_YH0) * 10e-55 # E[f(y | H1, D_space)] for y generated from H0
```

- And hence the expected posterior probability of $H_0$ from data $y$ generated from $H_0$, i.e. $E[P(H_0 | y, \mathbf{D}) | H_0, \mathbf{D}]$, is 
```{r}
# MED
fast_logsimpostH0_YH0 = log(fast_simEvidenceH0_YH0) - log(fast_simEvidenceH0_YH0 
                                                          + fast_simEvidenceH1_YH0)
fast_expected_postH0_YH0 = (1 / numSims) * exp(logSumExp(fast_logsimpostH0_YH0))
# space-filling
space_logsimpostH0 = log(space_simEvidenceH0_YH0) - log(space_simEvidenceH0_YH0 
                                                        + space_simEvidenceH1_YH0)
space_expected_postH0_YH0 = (1 / numSims) * exp(logSumExp(space_logsimpostH0))
# E[P(H0|Y,D)|H0,D]
mean(fast_expected_postH0_YH0) # posterior prob of H0
mean(space_expected_postH0_YH0) # posterior prob of H0
```

- which aren't what we want them to be.

# Figuring Out Why

## Do we need to average over the design space? I don't know. I don't think so.

# Even Simpler Design: Singleton

- The design with just the point $\mathbf{x}_0 = (0,0)$
- The design with just the point $\mathbf{x}_1 = (0.5,0.5)$

```{r}
x0 = matrix(c(0,0), 1, 2)
x1 = matrix(c(0.5,0.5), 1, 2)
x0_exppostprobs = suppressWarnings(calcExpPostProbH_2d(x0, 1, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
x1_exppostprobs = suppressWarnings(calcExpPostProbH_2d(x1, 1, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbsHi2 = data.frame(cbind(x0_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")],
                                             x1_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")]))
colnames(expectedPosteriorProbsHi2) = c("x0", "x1")
rownames(expectedPosteriorProbsHi2) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H1,D]")
round(expectedPosteriorProbsHi2,5)
```

- Why isn't $E[P(H_0 | y, \mathbf{x}_1) | H_0, \mathbf{x}_1]$ a lot higher than $E[P(H_0 | y, \mathbf{x}_0) | H_0, \mathbf{x}_0]$?

- Looking at the evidence calculation,

```{r}
# x0 = (0, 0)
x0_simY_H0 = simulateY(x0, 1, mean_beta0, var_mean0, var_e, numSims, type = type[1])
x0_simEvidenceH0_YH0 = rep(NA, numSims) # added for bf01 calc
x0_simEvidenceH1_YH0 = rep(NA, numSims)
for(j in 1:numSims){
  Y = x0_simY_H0[, j]
  x0_simEvidenceH0_YH0[j] = model_evidence(Y, x0, 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
  x0_simEvidenceH1_YH0[j] = model_evidence(Y, x0, 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
}
# x1 = (0.5, 0.5)
x1_simY_H0 = simulateY(x1, 1, mean_beta0, var_mean0, var_e, numSims, type = type[1])
x1_simEvidenceH0_YH0 = rep(NA, numSims) # added for bf01 calc
x1_simEvidenceH1_YH0 = rep(NA, numSims)
for(j in 1:numSims){
  Y = x1_simY_H0[, j]
  x1_simEvidenceH0_YH0[j] = model_evidence(Y, x1, 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
  x1_simEvidenceH1_YH0[j] = model_evidence(Y, x1, 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
}
```

- Comparing evidences:
- $E[f(y | H_0, \mathbf{D})]$.
```{r}
mean(x1_simEvidenceH0_YH0) > mean(x0_simEvidenceH0_YH0)
mean(x1_simEvidenceH0_YH0) # E[f(y | H0, D1)] for y generated from H0
mean(x0_simEvidenceH0_YH0) # E[f(y | H0, D0)] for y generated from H0
```

- $E[f(y | H_1, \mathbf{D})]$.
```{r}
mean(x1_simEvidenceH1_YH0) > mean(x0_simEvidenceH1_YH0)
mean(x1_simEvidenceH1_YH0) # E[f(y | H1, D1)] for y generated from H0
mean(x0_simEvidenceH1_YH0) # E[f(y | H1, D0)] for y generated from H0
```

- For some reason, $E[f(y | H_0, \mathbf{x}_1)] < E[f(y | H_0, \mathbf{x}_0)]$. What is wrong with the likelihood calculation?

- First, check simulations:

```{r}
xy_plot = rbind(matrix(rep((x0), numSims), numSims, 2), 
                matrix(rep((x1), numSims), numSims, 2))
z_plot = c(x0_simY_H0, x1_simY_H0)
scatterplot3d(x = xy_plot[ , 1], y = xy_plot[ , 2], z = z_plot,
              xlim = c(0, 1), ylim = c(0, 1), color = 2,
              xlab = "x1", ylab = "x2", zlab = "y", main = "Y | H0 Simulations")
```

- They seem fine.

- Check the evidence calculation then:
```{r}
# model_evidence = function(Y, D, N, mean_beta, var_mean, var_e, type){
#   # Y is a vector
#   # X is a matrix
#   # var_mean is a matrix
#   # var_e is a scalar
#   if(N != length(Y)) stop("N is not the same length as Y")
#   X = constructDesignX(D, N, type)
#   marginaly_mean = X %*% mean_beta
#   if(dim(X)[1] > 1){
#     marginaly_var = diag(rep(var_e, N)) + (X %*% var_mean %*% t(X))
#   } else{
#     marginaly_var = var_e + (X %*% var_mean %*% t(X))
#   }
#   return(dmvnorm(Y, mean = marginaly_mean, sigma = marginaly_var, log = FALSE))
# }
```


- for x0, the evidences should be the same.
```{r}
index = 5 # arbitrarily picked index of simulated Y | H0
x0_y_H0 = x0_simY_H0[, index]
x0_y_H0

# evidence for H0:
x0_X_H0 = constructDesignX(x0, 1, type[1])
x0_marginaly_mean_H0 = x0_X_H0 %*% mean_beta0
x0_marginaly_var_H0 = var_e + (x0_X_H0 %*% var_mean0 %*% t(x0_X_H0))
x0_evidence_H0 = dmvnorm(x0_y_H0, mean = x0_marginaly_mean_H0, sigma = x0_marginaly_var_H0, log = FALSE)
# should be same as
x0_evidence2_H0 = dnorm(x0_y_H0, mean = x0_marginaly_mean_H0, sd = sqrt(x0_marginaly_var_H0), log = FALSE)
round(x0_evidence_H0, 6) == round(x0_evidence2_H0, 6) # and they are the same.
x0_evidence_H0

# evidence for H1:
x0_X_H1 = constructDesignX(x0, 1, type[2])
x0_marginaly_mean_H1 = x0_X_H1 %*% mean_beta1
x0_marginaly_var_H1 = var_e + (x0_X_H1 %*% var_mean1 %*% t(x0_X_H1))
x0_evidence_H1 = dmvnorm(x0_y_H0, mean = x0_marginaly_mean_H1, sigma = x0_marginaly_var_H1, log = FALSE)
# should be same as
x0_evidence2_H1 = dnorm(x0_y_H0, mean = x0_marginaly_mean_H1, sd = sqrt(x0_marginaly_var_H1), log = FALSE)
round(x0_evidence_H1, 6) == round(x0_evidence2_H1, 6) # and they are the same.
x0_evidence_H1
```

- for x1, the evidences should not be the same. the evidence for H0 should be larger than that for H1.
```{r}
index = 5 # arbitrarily picked index of simulated Y | H0
x1_y_H0 = x1_simY_H0[, index]
x1_y_H0

# evidence for H0:
x1_X_H0 = constructDesignX(x1, 1, type[1])
x1_marginaly_mean_H0 = x1_X_H0 %*% mean_beta0
x1_marginaly_var_H0 = var_e + (x1_X_H0 %*% var_mean0 %*% t(x1_X_H0))
x1_evidence_H0 = dmvnorm(x1_y_H0, mean = x1_marginaly_mean_H0, sigma = x1_marginaly_var_H0, log = FALSE)
# should be same as
x1_evidence2_H0 = dnorm(x1_y_H0, mean = x1_marginaly_mean_H0, sd = sqrt(x1_marginaly_var_H0), log = FALSE)
round(x1_evidence_H0, 6) == round(x1_evidence2_H0, 6) # and they are the same.
x1_evidence_H0

# evidence for H1:
x1_X_H1 = constructDesignX(x1, 1, type[2])
x1_marginaly_mean_H1 = x1_X_H1 %*% mean_beta1
x1_marginaly_var_H1 = var_e + (x1_X_H1 %*% var_mean1 %*% t(x1_X_H1))
x1_evidence_H1 = dmvnorm(x1_y_H0, mean = x1_marginaly_mean_H1, sigma = x1_marginaly_var_H1, log = FALSE)
# should be same as
x1_evidence2_H1 = dnorm(x1_y_H0, mean = x1_marginaly_mean_H1, sd = sqrt(x1_marginaly_var_H1), log = FALSE)
round(x1_evidence_H1, 6) == round(x1_evidence2_H1, 6) # and they are the same.
x1_evidence_H1
```

- for other indices...

- for x0, the evidences should be the same.
```{r}
index = 8 # arbitrarily picked index of simulated Y | H0
x0_y_H0 = x0_simY_H0[, index]
x0_y_H0

# evidence for H0:
x0_X_H0 = constructDesignX(x0, 1, type[1])
x0_marginaly_mean_H0 = x0_X_H0 %*% mean_beta0
x0_marginaly_var_H0 = var_e + (x0_X_H0 %*% var_mean0 %*% t(x0_X_H0))
x0_evidence_H0 = dmvnorm(x0_y_H0, mean = x0_marginaly_mean_H0, sigma = x0_marginaly_var_H0, log = FALSE)
# should be same as
x0_evidence2_H0 = dnorm(x0_y_H0, mean = x0_marginaly_mean_H0, sd = sqrt(x0_marginaly_var_H0), log = FALSE)
round(x0_evidence_H0, 6) == round(x0_evidence2_H0, 6) # and they are the same.
x0_evidence_H0

# evidence for H1:
x0_X_H1 = constructDesignX(x0, 1, type[2])
x0_marginaly_mean_H1 = x0_X_H1 %*% mean_beta1
x0_marginaly_var_H1 = var_e + (x0_X_H1 %*% var_mean1 %*% t(x0_X_H1))
x0_evidence_H1 = dmvnorm(x0_y_H0, mean = x0_marginaly_mean_H1, sigma = x0_marginaly_var_H1, log = FALSE)
# should be same as
x0_evidence2_H1 = dnorm(x0_y_H0, mean = x0_marginaly_mean_H1, sd = sqrt(x0_marginaly_var_H1), log = FALSE)
round(x0_evidence_H1, 6) == round(x0_evidence2_H1, 6) # and they are the same.
x0_evidence_H1
```

- for x1, the evidences should not be the same. the evidence for H0 should be larger than that for H1.
```{r}
index = 8 # arbitrarily picked index of simulated Y | H0
x1_y_H0 = x1_simY_H0[, index]
x1_y_H0

# evidence for H0:
x1_X_H0 = constructDesignX(x1, 1, type[1])
x1_marginaly_mean_H0 = x1_X_H0 %*% mean_beta0
x1_marginaly_var_H0 = var_e + (x1_X_H0 %*% var_mean0 %*% t(x1_X_H0))
x1_evidence_H0 = dmvnorm(x1_y_H0, mean = x1_marginaly_mean_H0, sigma = x1_marginaly_var_H0, log = FALSE)
# should be same as
x1_evidence2_H0 = dnorm(x1_y_H0, mean = x1_marginaly_mean_H0, sd = sqrt(x1_marginaly_var_H0), log = FALSE)
round(x1_evidence_H0, 6) == round(x1_evidence2_H0, 6) # and they are the same.
x1_evidence_H0

# evidence for H1:
x1_X_H1 = constructDesignX(x1, 1, type[2])
x1_marginaly_mean_H1 = x1_X_H1 %*% mean_beta1
x1_marginaly_var_H1 = var_e + (x1_X_H1 %*% var_mean1 %*% t(x1_X_H1))
x1_evidence_H1 = dmvnorm(x1_y_H0, mean = x1_marginaly_mean_H1, sigma = x1_marginaly_var_H1, log = FALSE)
# should be same as
x1_evidence2_H1 = dnorm(x1_y_H0, mean = x1_marginaly_mean_H1, sd = sqrt(x1_marginaly_var_H1), log = FALSE)
round(x1_evidence_H1, 6) == round(x1_evidence2_H1, 6) # and they are the same.
x1_evidence_H1
```

- so far, it looks fine. something must go wrong when there are more than one design points, then.
- is it because I'm supposed to be multiplying the likelihoods together...? instead of using multivariate normal distribution...?


## Two Other Designs

- To understand why this is happening, let's look at the much simpler designs $\mathbf{D}_0 = \{ (0, 0), \dots, (0, 0) \}$ and $\mathbf{D}_1 = \{ (0.5, 0.5), \dots, (0.5, 0.5) \}$.
- At $\mathbf{D}_0$, the simulated Y should have a hard time distinguishing the models, whereas $\mathbf{D}_1$ should have the easiest time, since it is located where the models are most different from each other.

```{r, echo=FALSE}
newN = 100
d0 = matrix(rep(0, 3*2), newN, 2)
d1 = matrix(rep(0.5, newN*2), newN, 2)
d0_exppostprobs = suppressWarnings(calcExpPostProbH_2d(d0, newN, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
d1_exppostprobs = suppressWarnings(calcExpPostProbH_2d(d1, newN, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbsHi2 = data.frame(cbind(d0_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")],
                                          d1_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")]))
colnames(expectedPosteriorProbsHi2) = c("D0", "D1")
rownames(expectedPosteriorProbsHi2) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H1,D]")
round(expectedPosteriorProbsHi2,5)
```

- These results make me certain that the issue is in the calculation.

- Looking at the evidence calculation,

```{r}
# D0, (0, 0)s
d0_simY_H0 = simulateY(d0, newN, mean_beta0, var_mean0, var_e, numSims, type = type[1])
d0_simEvidenceH0_YH0 = matrix(rep(NA, numSims*newN), newN, numSims) # added for bf01 calc
d0_simEvidenceH1_YH0 = matrix(rep(NA, numSims*newN), newN, numSims)
for(j in 1:numSims){
  for(i in 1:newN){
    Y = d0_simY_H0[i, j]
    d0_simEvidenceH0_YH0[i, j] = model_evidence(Y, d0[i, , drop = FALSE], 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
    d0_simEvidenceH1_YH0[i, j] = model_evidence(Y, d0[i, , drop = FALSE], 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
  }
}
# D1, (0.5, 0.5)s
d1_simY_H0 = simulateY(d1, newN, mean_beta0, var_mean0, var_e, numSims, type = type[1])
d1_simEvidenceH0_YH0 = matrix(rep(NA, numSims*newN), newN, numSims) # added for bf01 calc
d1_simEvidenceH1_YH0 = matrix(rep(NA, numSims*newN), newN, numSims)
for(j in 1:numSims){
  for(i in 1:newN){
    Y = d1_simY_H0[i, j]
    d1_simEvidenceH0_YH0[i, j] = model_evidence(Y, d1[i, , drop = FALSE], 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
    d1_simEvidenceH1_YH0[i, j] = model_evidence(Y, d1[i, , drop = FALSE], 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
  }
}
mean(apply(d1_simEvidenceH0_YH0, 2, prod))
mean(apply(d1_simEvidenceH1_YH0, 2, prod))

```

- And looking at some comparisons,

## Comparing Evidences from Each Hypothesis Model, Given Data

- Surprisingly, they are similar for $E[f(y | H_0, \mathbf{D})]$. Is this because the null hypothesis is true, so their evidences 
```{r}
mean(d1_simEvidenceH0_YH0) > mean(d0_simEvidenceH0_YH0)
mean(d1_simEvidenceH0_YH0) # E[f(y | H0, D1)] for y generated from H0
mean(d0_simEvidenceH0_YH0) # E[f(y | H0, D0)] for y generated from H0
```

- And unlike with the comparison between MED and space-filling design, $E[f(y | H_{1}, \mathbf{D}_{1})] < E[f(y | H_{1}, \mathbf{D}_{0})]$. This seems right, and it also makes sense that $E[f(y | H_{1}, \mathbf{D}_{0})] \approx E[f(y | H_{0}, \mathbf{D}_{0})]$ above, since it cannot distinguish between $H_0$ and $H_1$ very well at the point $\mathbf{x} = (0, 0)$, where $f_0(\mathbf{x}) = f_1(\mathbf{x})$.
```{r}
mean(d1_simEvidenceH1_YH0) > mean(d0_simEvidenceH1_YH0)
mean(d1_simEvidenceH1_YH0) # E[f(y | H1, D1)] for y generated from H0
mean(d0_simEvidenceH1_YH0) # E[f(y | H1, D0)] for y generated from H0
```

- But why $E[P(H_0 | y, \mathbf{D}_{space}) | H_0, \mathbf{D}_{space}] > E[P(H_0 | y, \mathbf{D}_1) | H_0, \mathbf{D}_1]$? 

- Comparing $\mathbf{D}_1$ to $\mathbf{D}_{space}$, 

- $E[f(y | H_0, \mathbf{D})]$: it looks that the evidence for $\mathbf{D}_1$ is higher.
```{r}
mean(d1_simEvidenceH0_YH0) > mean(space_simEvidenceH0_YH0)
mean(d1_simEvidenceH0_YH0) * 10e-57 # E[f(y | H0, D1)] for y generated from H0
mean(space_simEvidenceH0_YH0) * 10e-57 # E[f(y | H0, D_space)] for y generated from H0
```

- $E[f(y | H_1, \mathbf{D})]$: it looks that the evidence for $\mathbf{D}_1$ is higher for $H_1$ as well.
```{r}
mean(d1_simEvidenceH1_YH0) > mean(space_simEvidenceH1_YH0)
mean(d1_simEvidenceH1_YH0) * 10e-57 # E[f(y | H1, D1)] for y generated from H0
mean(space_simEvidenceH1_YH0) * 10e-57 # E[f(y | H1, Dspace)] for y generated from H0
```

- Perhaps it doesn't do any good to compare these likelihood values, though, since they aren't normalized.

- Comparing Average Posterior Probabilities of Hypotheses:
    - The expected posterior probability of $H_0$ from data $y$ generated from $H_0$, i.e. $E[P(H_0 | y, \mathbf{D}) | H_0, \mathbf{D}]$ for $\mathbf{D}_0$ and $\mathbf{D}_1$ are calculated as follows:
```{r}
# MED
d0_logsimpostH0_YH0 = log(d0_simEvidenceH0_YH0) - log(d0_simEvidenceH0_YH0 
                                                      + d0_simEvidenceH1_YH0)
d0_expected_postH0_YH0 = (1 / numSims) * exp(logSumExp(d0_logsimpostH0_YH0))
# space-filling
d1_logsimpostH0 = log(d1_simEvidenceH0_YH0) - log(d1_simEvidenceH0_YH0 
                                                  + d1_simEvidenceH1_YH0)
d1_expected_postH0_YH0 = (1 / numSims) * exp(logSumExp(d1_logsimpostH0))
# E[P(H0|Y,D)|H0,D]
mean(d0_expected_postH0_YH0) # posterior prob of H0
mean(d1_expected_postH0_YH0) # posterior prob of H0
```



# Looking at D-Optimal Design

```{r, echo = FALSE}
numSims = 100
fast_exppostprobs = suppressWarnings(calcExpPostProbH_2d(fast_S5, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
space_exppostprobs = suppressWarnings(calcExpPostProbH_2d(space_filling, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
dopt_exppostprobs = suppressWarnings(calcExpPostProbH_2d(doptimal, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbsHi = data.frame(cbind(fast_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")], 
                                          space_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")],
                                          dopt_exppostprobs[c("expected_postH0_YH0", "expected_postH1_YH1")]))
colnames(expectedPosteriorProbsHi) = c("Fast","Space", "DOpt")
rownames(expectedPosteriorProbsHi) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H1,D]")
round(expectedPosteriorProbsHi,5)
```

- (In case you're wondering,) the full table of posterior probabilities is:
```{r, echo = FALSE}
numSims = 100
#fast_exppostprobs = suppressWarnings(calcExpPostProbH_2d(fast_S5, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
#space_exppostprobs = suppressWarnings(calcExpPostProbH_2d(space_filling, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
#dopt_exppostprobs = suppressWarnings(calcExpPostProbH_2d(doptimal, N, mean_beta0, mean_beta1, var_mean0, var_mean1, var_e, numSims, type = type))
expectedPosteriorProbs = data.frame(cbind(fast_exppostprobs, space_exppostprobs, dopt_exppostprobs, d0_exppostprobs, d1_exppostprobs))
colnames(expectedPosteriorProbs) = c("Fast","Space", "DOpt", "D0", "D1")
rownames(expectedPosteriorProbs) = c("E[P(H0|Y,D)|H0,D]", "E[P(H1|Y,D)|H0,D]", "E[BF01 | H0,D]", 
                                     "E[P(H0|Y,D)|H1,D]", "E[P(H1|Y,D)|H1,D]", "E[BF01|H1,D]")
round(expectedPosteriorProbs,4)
```

- Why isn't Doptimal 0.5, 0.5?
```{r}
# DOpt
dopt_simY_H0 = simulateY(doptimal, N, mean_beta0, var_mean0, var_e, numSims, type = type[1])
dopt_simEvidenceH0_YH0 = matrix(rep(NA, numSims*N), N, numSims) # added for bf01 calc
dopt_simEvidenceH1_YH0 = matrix(rep(NA, numSims*N), N, numSims)
for(j in 1:numSims){
  for(i in 1:N){
    Y = dopt_simY_H0[i, j]
    dopt_simEvidenceH0_YH0[i, j] = model_evidence(Y, doptimal[i, , drop = FALSE], 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
    dopt_simEvidenceH1_YH0[i, j] = model_evidence(Y, doptimal[i, , drop = FALSE], 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
  }
}
# DOpt
dopt_simY_H1 = simulateY(doptimal, N, mean_beta1, var_mean1, var_e, numSims, type = type[2])
dopt_simEvidenceH0_YH1 = matrix(rep(NA, numSims*N), N, numSims) # added for bf01 calc
dopt_simEvidenceH1_YH1 = matrix(rep(NA, numSims*N), N, numSims)
for(j in 1:numSims){
  for(i in 1:N){
    Y = dopt_simY_H1[i, j]
    dopt_simEvidenceH0_YH1[i, j] = model_evidence(Y, doptimal[i, , drop = FALSE], 1, mean_beta0, 
                                           var_mean0, var_e, type = type[1])
    dopt_simEvidenceH1_YH1[i, j] = model_evidence(Y, doptimal[i, , drop = FALSE], 1, mean_beta1, 
                                           var_mean1, var_e, type = type[2])
  }
}
mean(apply(dopt_simEvidenceH0_YH0, 2, prod)) * 10e-43
mean(apply(dopt_simEvidenceH1_YH0, 2, prod)) * 10e-43
mean(apply(dopt_simEvidenceH0_YH1, 2, prod)) * 10e-43
mean(apply(dopt_simEvidenceH1_YH1, 2, prod)) * 10e-43
```

- why aren't they equal?
```{r}
xy_plot = rbind(doptimal, doptimal)
z_plot = c(dopt_simY_H0, dopt_simY_H1)
scatterplot3d(x = xy_plot[ , 1], y = xy_plot[ , 2], z = z_plot,
              xlim = c(0, 1), ylim = c(0, 1), color = 2,
              xlab = "x1", ylab = "x2", zlab = "y", main = "Y | H0 Simulations")
```









